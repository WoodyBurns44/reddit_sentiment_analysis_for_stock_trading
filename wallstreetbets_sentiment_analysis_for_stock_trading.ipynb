{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reddit WallStreetBets Sentiment Analysis for use in Stock Trading "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports for processing Data, performing calculations, and plotting\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sn\n",
    "\n",
    "\n",
    "# Reddits API for compiling posts\n",
    "#!pip install praw --upgrade\n",
    "import praw #reddit data api\n",
    "\n",
    "#Used for compiling stock information\n",
    "#!pip install ffn\n",
    "import ffn \n",
    "\n",
    "\n",
    "# RegEx \n",
    "import re #regex\n",
    "!pip install vaderSentiment\n",
    "\n",
    "# VADER used for social media sentiment analysis\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer #VADER sentiment model\n",
    "\n",
    "# Used to store and pull in large DataFrames that are compiled\n",
    "import requests\n",
    "import json\n",
    "import csv\n",
    "\n",
    "# For processing time functions\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "# For Regularization\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "pd.set_option('display.max_rows', 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import TensorFlow for building a neural network\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "tf.test.gpu_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to compile post data for use in sentiment Analysis\n",
    "def getData(query, after, before, sub):\n",
    "    url = 'https://api.pushshift.io/reddit/search/submission/?title='+str(query)+'&size=1000&after='+str(after)+'&before='+str(before)+'&subreddit='+str(sub)\n",
    "    print(url)\n",
    "    red = requests.get(url)\n",
    "    data = json.loads(red.text)\n",
    "    return data['data']\n",
    "\n",
    "# Get relevant data from data extracted using previous function\n",
    "def getSubData(subm):\n",
    "    subData = [subm['id'], subm['title'], subm['url'], datetime.datetime.fromtimestamp(subm['created_utc']).date()]\n",
    "    try:\n",
    "        flair = subm['link_flair_text']\n",
    "    except KeyError:\n",
    "        flair = \"NaN\"\n",
    "    subData.append(flair)\n",
    "    stats.append(subData)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Indicate which subreddit to pull form \n",
    "\n",
    "sub = 'wallstreetbets'\n",
    "\n",
    "# Setting time range\n",
    "# Setting time range for all of 2021 so far \n",
    "\n",
    "before = \"1615827608\" #March 15 2021\n",
    "after = \"1609520408\" #January 1 2021\n",
    "\n",
    "# Set query string\n",
    "query = \"Daily Discussion Thread\"\n",
    "count = 0\n",
    "stats = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather all data between the desired dates\n",
    "data = getData(query, after, before, sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile Submissions\n",
    "while len(data) > 0:\n",
    "    for submission in data:\n",
    "        getSubData(submission)\n",
    "        count+=1\n",
    "        \n",
    "    print(len(data))\n",
    "    print(str(datetime.datetime.fromtimestamp(data[-1]['created_utc'])))\n",
    "    after = data[-1]['created_utc']\n",
    "    reddit = getData(query, after, before, sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put data into DataFrame\n",
    "\n",
    "data = {}\n",
    "ids = []\n",
    "titles = []\n",
    "urls = []\n",
    "dates = []\n",
    "flairs = []\n",
    "\n",
    "for stat in stats:\n",
    "    ids.append(stat[0])\n",
    "    titles.append(stat[1]) \n",
    "    urls.append(stat[2])\n",
    "    dates.append(stat[3])\n",
    "    flairs.append(stat[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame\n",
    "\n",
    "reddit['id']=ids\n",
    "reddit['title']=titles\n",
    "reddit['url']=urls\n",
    "reddit['date']=dates\n",
    "reddit['flair']=flairs\n",
    "\n",
    "reddit_df = pd.DataFrame(reddit)\n",
    "reddit_df = reddit_df[reddit_df['flair'] == 'Daily Discussion']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Reddit connection\n",
    "# Withholding these fields for privacy. \n",
    "reddit = praw.Reddit(client_id='', client_secret='', user_agent='')\n",
    "\n",
    "# Collect Reddit Comments\n",
    "daily_comments=[]\n",
    "for url in reddit_df['url'].tolist():\n",
    "    try:\n",
    "        submission = reddit.submission(url=url)\n",
    "        submission.comments.replace_more(limit=0)\n",
    "        comments=list([(comment.body) for comment in submission.comments])\n",
    "    except:\n",
    "        comments=None\n",
    "    daily_comments.append(comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Vader sentiment analyzer to get sentiment score on posts\n",
    "\n",
    "vader = SentimentIntensityAnalyzer()\n",
    "\n",
    "scores = []\n",
    "for comment in daily_comments:\n",
    "    sent_score = 0\n",
    "    try:\n",
    "        for comment in comments:\n",
    "            sent_score = sent_score + vader.polarity_scores(comment)['compound']\n",
    "    except TypeError:\n",
    "        sent_score = 0\n",
    "        \n",
    "    scores.append(sent_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plug score sinto DataFrame\n",
    "reddit_df['sentiment score'] = scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve stock prices for SPY - S&P 500 aggregate Stock\n",
    "\n",
    "spy = ffn.get('spy', start = '2021-01-01')\n",
    "spy_values = []\n",
    "\n",
    "for date in reddit_df['date'].tolist():\n",
    "    try:\n",
    "        spy_value.append(float(spy.loc[date]))\n",
    "    except KeyError:\n",
    "        spy_values.append(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Submit stock prices to original DataFrame\n",
    "reddit_df['spy'] = spy_values\n",
    "\n",
    "# Isolate Columns of INterest \n",
    "reddit_df = reddit_df[['date', 'sentiment score', 'spy']]\n",
    "\n",
    "reddit_df = reddit_df.set_index('date')\n",
    "reddit_df = reddit_df[reddit_df['spy'].notna()]\n",
    "\n",
    "reddit_df.to_csv('positive+negative_sentiment_data_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Data to CSV so I dont have to run this long function everytime\n",
    "reddit_df = pd.read_csv('positive+negative_sentiment_data_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot of sentiment score vs. SPY price\n",
    "\n",
    "reddit_df.plot(secondary_y = 'sentiment score', figsize = (15, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Frouier to remove noise from sentminet score\n",
    "\n",
    "open_fft = np.fft.fft(np.asarray(reddit_df['sentiment score'].tolist()))\n",
    "\n",
    "fourier_df = pd.DataFrame({'fft': open_fft})\n",
    "\n",
    "fourier_df['absolute'] = fourier_df['fft'].apply(lambda x: np.abs(x))\n",
    "fourier_df['angular'] = fourier_df['fft'].apply(lambda x: np.angle(x))\n",
    "\n",
    "fourier_list = np.asarray(fourier_df['fft'].tolist())\n",
    "\n",
    "for num in [5, 10, 20]:\n",
    "    fourier_list_2 = np.copy(fourier_list); fourier_list_2[num: -num] = 0\n",
    "    reddit_df['fourier ' + str(num)] = np.fft.ifft(fft_list_2)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot of Sentiment Score, as well as Fourier flattened with 5, 10, and 20 frequencies\n",
    "reddit_df[['sentiment score', 'fourier 5', 'fourier 10', 'fourier 20']].plot(figsize=(16, 10));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot of the SPY price and with fourier at 20 frequency\n",
    "reddit_df[['spy', 'fourier 20']].plot(secondary_y = 'fourier 20', figsize=(16, 10));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing the data\n",
    "\n",
    "scaler = MinMaxScaler(feature_range = (0, 1))\n",
    "\n",
    "reddit_df['normalized price'] = scaler.fit_transform(reddit_df['spy'].to_numpy().reshape(-1, 1))\n",
    "\n",
    "reddit_df['log spy'] = np.log(reddit_df['spy'] / reddit_df['spy'].shift(1))\n",
    "\n",
    "reddit_df['normalized sentiment'] = scaler.fit_transform(reddit_df['sentiment score'].to_numpy().reshape(-1, 1))\n",
    "\n",
    "reddit_df['normalized fourier 5'] = scaler.fit_transform(np.asarray(list([(float(x)) for x in reddit_df['fourier 5'].to_numpy()])).reshape(-1, 1))\n",
    "reddit_df['normalized fourier 10'] = scaler.fit_transform(np.asarray(list([(float(x)) for x in reddit_df['fourier 10'].to_numpy()])).reshape(-1, 1))\n",
    "reddit_df['normalized fourier 20'] = scaler.fit_transform(np.asarray(list([(float(x)) for x in reddit_df['fourier 20'].to_numpy()])).reshape(-1, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# Plot of all normalized Data\n",
    "reddit_df[['normalized price', 'normalized sentiment', 'normalized fourier 5', 'normalized fourier 10', 'normalized fourier 20']].plot(figsize=(16, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SPY Price vs. +/- Sentiment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Correlation \n",
    "\n",
    "four_vs_price_corr = reddit_df[['normalized fourier 20', 'normalized_price']].corr()\n",
    "four_vs_price_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame with rolling correlation\n",
    "\n",
    "two_week_rolling = reddit_df['normalized_price'].rolling(window=14).corr(reddit_df['normalized fourier 20'])\n",
    "\n",
    "corr = reddit_df[['normalized fourier 20', 'normalized_price']].corr().iloc[0, 1]\n",
    "\n",
    "mean = two_week_rolling.mean()\n",
    "print(mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Standard Deviation of two week rolling average\n",
    "std = np.std(two_week_rolling)\n",
    "print(std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the two week rolling correlation with actual correlation\n",
    "ax = two_week_rolling.plot(figsize=(16, 10))\n",
    "\n",
    "ax.axhline(corr, c='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot two week rolling correlation vs. normalized SPY price \n",
    "# with lines at mean and mean minus std of rolling correlation. \n",
    "\n",
    "reddit_df['two week rolling'] = two_week_rolling\n",
    "\n",
    "ax = reddit_df[['two_week_rolling', 'normalized_price']].plot(secondary_y = 'normalized_price', figsize=(16, 10))\n",
    "\n",
    "ax.axhline(mean-std, c='black');\n",
    "ax.axhline(mean, c='red');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bullish and Bearish Sentiment Analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will be following the same approach as before, but this time I only need the Title of the Reddit Post to determine whether the statement is Bear-ish or Bullish. From There I will build a Neural Network to attempt to predict the next days Stock price. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subreddit to scrape\n",
    "sub = 'wallstreetbets'\n",
    "\n",
    "# Date Range \n",
    "\n",
    "before = \"1615827608\" #March 15 2021\n",
    "after = \"1609520408\" #January 1 2021\n",
    "query = \"\"\n",
    "\n",
    "count = 0\n",
    "stats = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Data from the subreddit posts between the time frame indicated\n",
    "reddit2 = getData(query, after, before, sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile Posts\n",
    "\n",
    "while len(reddit2) > 0:\n",
    "    for submission in reddit2:\n",
    "        getSubData(submission)\n",
    "        count += 1\n",
    "        \n",
    "        print(len(reddit2))\n",
    "        print(str(datetime.datetime.fromtimestamp(reddit2[-1]['created_utc'])))\n",
    "        \n",
    "        after = reddit2[-1]['created_utc']\n",
    "        \n",
    "        try:\n",
    "            reddit2 = getData(query, after, before, sub)\n",
    "        \n",
    "        except:\n",
    "            pass\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all relevant information\n",
    "reddit2 = {}\n",
    "\n",
    "ids = []\n",
    "titles = []\n",
    "urls = []\n",
    "dates = []\n",
    "flairs = []\n",
    "\n",
    "for stat in stats:\n",
    "    ids.append(stat[0])\n",
    "    titles.append(stat[1])\n",
    "    urls.append(stat[2])\n",
    "    dates.append(stat[3])\n",
    "    flairs.append(stat[4])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame of the information\n",
    "reddit2['id'] = ids\n",
    "reddit2['title'] = titles\n",
    "reddit2['url'] = urls\n",
    "reddit2['date'] = dates\n",
    "reddit2['flair'] = flairs\n",
    "\n",
    "reddit_df2 = pd.DataFrame(reddit2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract jsut the titles of the reddit posts. \n",
    "titles = reddit_df2['title'].tolist()\n",
    "titles = list([(title.lower()) for title in titles])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a list of words to indicate whether a post is Bull-ish or Bear-ish\n",
    "bullish = ['call', 'long', 'all in', 'moon', 'going up', 'rocket', 'buy', 'long term', 'green']\n",
    "bearish = ['put', 'short', 'going down', 'drop', 'bear', 'sell', 'red']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a scoring index to give to bearish and bullish posts \n",
    "bullish_scores = []\n",
    "bearish_scores = []\n",
    "\n",
    "for title in titles:\n",
    "    bull = False\n",
    "    bear = False\n",
    "    \n",
    "    for word in bullish:\n",
    "        if word in title:\n",
    "            bear = True\n",
    "    if re.findall(r'(\\b\\d{1,4}[c]\\b)|(\\b\\d{1,4}[ ][c]\\b)', title):\n",
    "        bull = True\n",
    "        \n",
    "    for word in bearish:\n",
    "        if word in title:\n",
    "            bear = True\n",
    "    if re.findall(r'(\\b\\d{1,4}[p]\\b)|(\\b\\d{1,4}[ ][p]\\b)', title):\n",
    "            bear = True\n",
    "            \n",
    "    \n",
    "    \n",
    "    if bull == True and bear == True:\n",
    "        bullish_scores.append(0)\n",
    "        bearish_scores.append(0)\n",
    "    if bull == False and bear == False:\n",
    "        bullish_scores.append(0)\n",
    "        bearish_scores.append(0)\n",
    "    if bull == True and bear == False:\n",
    "        bullish_scores.append(1)\n",
    "        bearish_scores.append(0)\n",
    "        \n",
    "    if bull == False and bear == True:\n",
    "        bullish_scores.append(0)\n",
    "        bearish_scores.append(1)\n",
    "        \n",
    "reddit_df2['bullish score'] = bullish_scores\n",
    "reddit_df2['bearish score'] = bearish_scores   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isolate posts with desired flairs\n",
    "\n",
    "ind = []\n",
    "\n",
    "flairs = reddit_df2['flair'].tolist()\n",
    "\n",
    "for i in range(len(flairs)):\n",
    "    if flairs[i] == 'DD' or flairs[i] =='Discussion' or flairs[i] == 'YOLO' or flairs[i] == 'Fundamentals' or flairs[i] == 'Stocks':\n",
    "        ind.append(i)\n",
    "\n",
    "        \n",
    "reddit_df2 = reddit_df2.iloc[ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum sentiment scores from all the submission titles per day \n",
    "# and divide by total submissions for that day\n",
    "\n",
    "scores_df = reddit_df2.groupby('date').sum()\n",
    "\n",
    "scores_df['bullish score'] == scores_df['bullish score'] / reddit_df2.groupby('date').count()['bullish score']\n",
    "scores_df['bearish score'] == scores_df['bearish score'] / reddit_df2.groupby('date').count()['bearish score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the SPY stock price for the time frame and assign the appropriate bear and bull scores\n",
    "\n",
    "reddit_df2 = ffn.get('spy', start = '2021-01-01')\n",
    "\n",
    "reddit_df2 = reddit_df2.loc[:'2021-03-14']\n",
    "\n",
    "bullish_values = []\n",
    "bearish_values = []\n",
    "\n",
    "for date in reddit_df2.index.tolist():\n",
    "    bullish_values.append(float(scores_df.loc[date.date()]['bullish score']))\n",
    "    bearish_values.append(float(scores_df.loc[date.date()]['bearish score']))   \n",
    "    \n",
    "\n",
    "reddit_df2['bullish score'] = bullish_values\n",
    "reddit_df2['bearish score'] = bearish_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to csv so I do not have to reoad the data everytime\n",
    "reddit_df2.to_csv('bull+bearish_sentiment_data_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the bull scores vs. SPY\n",
    "\n",
    "reddit_df2[['spy', 'bullish score']].loc['2021-01-01':].plot(secondary_y = 'bullish score', figsize=(16, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the bear scores vs. spy\n",
    "\n",
    "reddit_df2[['spy', 'bearish score']].loc['2021-01-01':].plot(secondary_y = 'bearish score', figsize=(16, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform Fourier transformaion to smooth data\n",
    "\n",
    "norm_fft = np.fft.fft(np.asarray(reddit_df2['bull score'].tolist()))\n",
    "fft_df = pd.DataFrame({'fft':norm_fft})\n",
    "fft_df['absolute'] = fft_df['fft'].apply(lambda x: np.abs(x))\n",
    "fft_df['angular'] = fft_df['fft'].apply(lambda x: np.angle(x))\n",
    "fft_list = np.asarray(fft_df['fft'].tolist())\n",
    "\n",
    "for num in [10, 30]:\n",
    "    fft_list_m10= np.copy(fft_list); fft_list_m10[num:-num]=0\n",
    "    reddit_df2['fourier bull '+str(num)]=np.fft.ifft(fft_list_m10)\n",
    "\n",
    "norm_fft = np.fft.fft(np.asarray(df_2['bear score'].tolist()))\n",
    "fft_df = pd.DataFrame({'fft':norm_fft})\n",
    "fft_df['absolute'] = fft_df['fft'].apply(lambda x: np.abs(x))\n",
    "fft_df['angular'] = fft_df['fft'].apply(lambda x: np.angle(x))\n",
    "fft_list = np.asarray(fft_df['fft'].tolist())\n",
    "\n",
    "for num in [10, 30]:\n",
    "    fft_list_m10= np.copy(fft_list); fft_list_m10[num:-num]=0\n",
    "    reddit_df2['fourier bear '+str(num)]=np.fft.ifft(fft_list_m10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot of Bullish scores with fourier transformation\n",
    "\n",
    "reddit_df2[['bullish score', 'fourier bull 10', 'fourier bull 30']].plot(figsize=(16, 10));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot of Bearish scores with fourier transformation\n",
    "\n",
    "reddit_df2[['bearish score', 'fourier bear 10', 'fourier bear 30']].plot(figsize=(16, 10));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# Normalize all of the variables\n",
    "\n",
    "scaler = MinMaxScaler(feature_range = (0, 1))\n",
    "\n",
    "redditdf_2['normalized_price']=scaler.fit_transform(redditdf_2['spy'].to_numpy().reshape(-1, 1))\n",
    "redditdf_2['log spy']=np.log(redditdf_2['spy']/redditdf_2['spy'].shift(1))\n",
    "redditdf_2['normalized_bull']=scaler.fit_transform(redditdf_2['bull score'].to_numpy().reshape(-1, 1))\n",
    "redditdf_2['normalized_bear']=sc.fit_transform(redditdf_2['bear score'].to_numpy().reshape(-1, 1))\n",
    "redditdf_2['normalized_fourier_bull_10']=sc.fit_transform(np.asarray(list([(float(x)) for x in redditdf_2['fourier bull 10'].to_numpy()])).reshape(-1, 1))\n",
    "redditdf_2['normalized_fourier_bear_10']=sc.fit_transform(np.asarray(list([(float(x)) for x in redditdf_2['fourier bear 10'].to_numpy()])).reshape(-1, 1))\n",
    "redditdf_2['normalized_fourier_bull_30']=sc.fit_transform(np.asarray(list([(float(x)) for x in redditdf_2['fourier bull 30'].to_numpy()])).reshape(-1, 1))\n",
    "redditdf_2['normalized_fourier_bear_30']=sc.fit_transform(np.asarray(list([(float(x)) for x in redditdf_2['fourier bear 30'].to_numpy()])).reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot of normalized spy price and fourier transformed bull sentiment scores \n",
    "\n",
    "reddit_df2[['normalized_price', 'normalized_fourier_bull_10', 'normalized_fourier_bull_30']].plot(figsize=(16, 10));\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot of normalized spy price and fourier transformed bear sentiment scores \n",
    "\n",
    "reddit_df2[['normalized_price', 'normalized_fourier_bear_10', 'normalized_fourier_bear_30']].plot(figsize=(16, 10));\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "source": [
    "## Neural Network to predict following days SPY price "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to manipulate numpy array\n",
    "def remove_first(array):\n",
    "    new_array = []\n",
    "    for x in array:\n",
    "        new_array.append(x[1:])\n",
    "    return np.asarray(new_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a 2 week window of data out of array. The array contains the window, number of days in each window, and the number of features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = reddit_df2[['normalized_price', \n",
    "                 'normalized_bull', \n",
    "                 'normalized_fourier_bull_10', \n",
    "                 'normalized_fourier_bear_10', \n",
    "                'normalized_fourier_bull_30',\n",
    "                'normalized_fourier_bear_30']].to_numpy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# Set windown, lag, and aggregate data\n",
    "window = 15\n",
    "gap = 1\n",
    "data = []\n",
    "\n",
    "for i in range(len(df)-window):\n",
    "    data.append(df[x:x+window])\n",
    "    \n",
    "data = np.asarray(data)\n",
    "\n",
    "train = data[:-50]\n",
    "test = data[-50]\n",
    "np.random.shuffle(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point I manually split the data into training and testing sets for validation and to run a neural network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training set\n",
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "for d in train:\n",
    "    X_train.append(remove_first(d[:window-gap]))\n",
    "    y_train.append(d[-1][0])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create testing set\n",
    "X_test = []\n",
    "y_test = []\n",
    "\n",
    "for d in test:\n",
    "    X_test.append(remove_first(d[:window-gap]))\n",
    "    y_test.append(d[-1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert train and testing into numpy arrays\n",
    "\n",
    "X_train = np.asarray(X_train)\n",
    "y_train = np.asarray(y_train)\n",
    "X_test = np.asarray(X_test)\n",
    "y_test = np.asarray(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I then contruct a a Long Short Term Memory neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "tf.random.set_seed(51)\n",
    "np.random.seed(51)\n",
    "\n",
    "mc = tf.keras.callbacks.ModelCheckpoint(filepath='lstm_bullbear_sentiment_1.h5', monitor='val_loss', save_best_only=True)\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.LSTM(48, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])),\n",
    "    tf.keras.layers.LSTM(48, return_sequences=True),\n",
    "    tf.keras.layers.LSTM(48),\n",
    "    tf.keras.layers.Dense(24, activation='relu'),\n",
    "    tf.keras.layers.Dense(12, activation='relu'),\n",
    "    tf.keras.layers.Dense(1),\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "history = model.fit(X_train, y_train, batch_size = 32, validation_data = [X_test, y_test], epochs = 250, callbacks = [mc]).history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = history['accuracy']\n",
    "val_accuracy = history['val_accuracy']\n",
    "loss = history['loss']\n",
    "\n",
    "val_loss = history['val_loss']\n",
    "epochs = range(len(acc))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 12))\n",
    "plt.plot(epochs, loss, 'r', label = 'Loss in Training')\n",
    "plt.plot(epochs, val_loss, 'b', label = 'Validation loss')\n",
    "plt.title(\"Training and Validation Loss\")\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(X_test)\n",
    "\n",
    "preds_df = pd.DataFrame(y_test, columns=['price'])\n",
    "preds_df['preds'] = preds\n",
    "\n",
    "pred_df.plot(figsize=(18, 12))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding the Mean Absolute Error of the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae = sum(tf.keras.metrics.mean_absolute_error(y_test, preds).numpy())/len(y_test)\n",
    "\n",
    "print(\"The MEA of the model is: \", mea)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
